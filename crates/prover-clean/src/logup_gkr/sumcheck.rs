//! Gkr is composed of rounds. Each round corresponds to a layer. Each round is a sumcheck.
//! For the first layer, the numerator is a base field element and the denominator over extension field elements, so it requires some special treatment.
use std::iter::once;

use csl_cuda::{
    args,
    sys::prover_clean::{
        logup_gkr_fix_and_sum_circuit_layer as fix_and_sum_circuit_layer_kernel,
        logup_gkr_fix_and_sum_interactions_layer as fix_and_sum_interactions_layer_kernel,
        logup_gkr_fix_and_sum_last_circuit_layer as fix_and_sum_last_circuit_layer_kernel,
        prover_clean_logup_gkr_first_sum_as_poly_circuit_layer as first_sum_as_poly_layer_circuit_layer_kernel,
        prover_clean_logup_gkr_fix_last_variable_first_layer as fix_last_variable_first_layer_kernel,
        prover_clean_logup_gkr_fix_last_variable_interactions_layer as fix_last_variable_interactions_layer_kernel,
        prover_clean_logup_gkr_fix_last_variable_last_circuit_layer as fix_last_row_last_circuit_layer_kernel,
        prover_clean_logup_gkr_sum_as_poly_circuit_layer as sum_as_poly_circuit_layer_kernel,
        prover_clean_logup_gkr_sum_as_poly_first_layer as sum_as_poly_first_layer_kernel,
    },
    TaskScope, ToDevice,
};
use itertools::Itertools;
use slop_algebra::{
    interpolate_univariate_polynomial, AbstractExtensionField, AbstractField, Field,
    UnivariatePolynomial,
};
use slop_alloc::{Buffer, HasBackend, ToHost};
use slop_challenger::FieldChallenger;
use slop_multilinear::{Mle, Point};
use slop_sumcheck::PartialSumcheckProof;
use slop_tensor::Tensor;

use crate::{
    logup_gkr::utils::{FirstLayerPolynomial, GkrLayer, LogupRoundPolynomial, PolynomialLayer},
    JaggedMle,
};
use crate::{
    logup_gkr::{
        layer::JaggedGkrLayer,
        utils::{generate_test_data, GkrTestData},
    },
    DenseData,
};
use rayon::prelude::*;
use slop_basefold::{BasefoldVerifier, Poseidon2KoalaBear16BasefoldConfig};
use slop_sumcheck::partially_verify_sumcheck_proof;

use crate::config::{Ext, Felt};

pub async fn get_component_poly_evals(poly: &LogupRoundPolynomial) -> Vec<Ext> {
    match &poly.layer {
        PolynomialLayer::InteractionsLayer(guts) => {
            debug_assert_eq!(guts.sizes(), [4, 1]);
            guts.as_buffer().to_host().await.unwrap().to_vec()
        }
        PolynomialLayer::CircuitLayer(_) => unreachable!(),
    }
}

/// Evaluates the first layer polynomial and eq polynomial at 0 and 1/2.
async fn sum_as_poly_first_layer(
    poly: &FirstLayerPolynomial,
    claim: Ext,
) -> UnivariatePolynomial<Ext> {
    let circuit = &poly.layer.jagged_mle;

    let height = circuit.dense_data.height >> 1;

    let scope = circuit.backend();

    const BLOCK_SIZE: usize = 256;
    const STRIDE: usize = 32;

    let grid_dim = height.div_ceil(BLOCK_SIZE).div_ceil(STRIDE);
    let mut output = Tensor::<Ext, TaskScope>::with_sizes_in([3, grid_dim], scope.clone());

    let num_tiles = BLOCK_SIZE.checked_div(STRIDE).unwrap_or(1);
    let shared_mem = num_tiles * std::mem::size_of::<Ext>();

    unsafe {
        output.assume_init();
        let args = args!(
            output.as_mut_ptr(),
            circuit.as_raw(),
            poly.eq_row.guts().as_ptr(),
            poly.eq_interaction.guts().as_ptr(),
            poly.lambda
        );
        scope
            .launch_kernel(
                sum_as_poly_first_layer_kernel(),
                grid_dim,
                BLOCK_SIZE,
                &args,
                shared_mem,
            )
            .unwrap();
    }
    let evals = output.sum(1).await.into_buffer().to_host().await.unwrap();

    let mut eval_zero: Ext = *evals[0];
    let mut eval_half: Ext = *evals[1];
    let eq_sum = *evals[2];

    // Correct the evaluations by the sum of the eq polynomial, which accounts for the
    // contribution of padded row for the denominator expression
    // `\Sum_i eq * denominator_0 * denominator_1`.
    let eq_correction_term = Ext::one() - eq_sum;
    // The evaluation at zero just gets the eq correction term.
    eval_zero += eq_correction_term * (Ext::one() - *poly.point.last().unwrap());
    // The evaluation at 1/2 gets the eq correction term times 4, since the denominators
    // have a 1/2 in them for the rest of the evaluations (so we multiply by 2 twice).
    eval_half += eq_correction_term * Ext::from_canonical_u16(4);

    // Since the sumcheck polynomial is homogeneous of degree 3, we need to divide by
    // 8 = 2^3 to account for the evaluations at 1/2 to be double their true value.
    let eval_half = eval_half * Ext::from_canonical_u16(8).inverse();

    // Get the root of the eq polynomial which gives an evaluation of zero.
    let point_last = poly.point.last().unwrap();
    let b_const = (Ext::one() - *point_last) / (Ext::one() - point_last.double());

    let eval_one = claim - eval_zero;
    interpolate_univariate_polynomial(
        &[
            Ext::from_canonical_u16(0),
            Ext::from_canonical_u16(1),
            Ext::from_canonical_u16(2).inverse(),
            b_const,
        ],
        &[eval_zero, eval_one, eval_half, Ext::zero()],
    )
}

/// Fix the last variable of the first gkr layer.
async fn fix_last_variable_first_layer(
    mut poly: FirstLayerPolynomial,
    alpha: Ext,
) -> LogupRoundPolynomial {
    let last_coordinate = poly.point.remove_last_coordinate();
    let padding_adjustment =
        last_coordinate * alpha + (Ext::one() - last_coordinate) * (Ext::one() - alpha);

    let backend = poly.layer.jagged_mle.backend();
    let height = poly.layer.jagged_mle.dense_data.height >> 1;
    // If this is not the last layer, we need to fix the last variable and create a
    // new circuit layer.
    let output_interaction_col_sizes = poly
        .layer
        .interaction_col_sizes
        .iter()
        .map(|count| count.div_ceil(4) * 2)
        .collect::<Vec<_>>();
    // The output indices is just the prefix sum of the interaction row counts.
    let output_interaction_start_indices = once(0)
        .chain(output_interaction_col_sizes.iter().scan(0u32, |acc, x| {
            *acc += x;
            Some(*acc)
        }))
        .collect::<Buffer<_>>();
    let output_height = output_interaction_start_indices.last().copied().unwrap() as usize;
    let output_interaction_start_indices =
        output_interaction_start_indices.to_device_in(backend).await.unwrap();

    // Create a new layer
    let output_layer: Tensor<Ext, TaskScope> =
        Tensor::with_sizes_in([4, 1, output_height * 2], backend.clone());
    let output_col_index: Buffer<u32, TaskScope> =
        Buffer::with_capacity_in(output_height, backend.clone());

    let output_jagged_layer = JaggedGkrLayer::new(output_layer, output_height);
    let mut output_jagged_mle =
        JaggedMle::new(output_jagged_layer, output_col_index, output_interaction_start_indices);

    // populate the new layer
    const BLOCK_SIZE: usize = 256;
    const STRIDE: usize = 32;
    let grid_size_x = height.div_ceil(BLOCK_SIZE * STRIDE);
    let grid_size = (grid_size_x, 1, 1);
    let block_dim = BLOCK_SIZE;
    unsafe {
        output_jagged_mle.dense_data.assume_init();
        output_jagged_mle.col_index.assume_init();
        let args = args!(poly.layer.jagged_mle.as_raw(), output_jagged_mle.as_mut_raw(), alpha);
        backend
            .launch_kernel(fix_last_variable_first_layer_kernel(), grid_size, block_dim, &args, 0)
            .unwrap();
    }
    // Fix the eq_row variables
    let eq_row = poly.eq_row.fix_last_variable(alpha).await;

    let output_layer = GkrLayer {
        jagged_mle: output_jagged_mle,
        interaction_col_sizes: output_interaction_col_sizes,
        num_row_variables: poly.layer.num_row_variables - 1,
        num_interaction_variables: poly.layer.num_interaction_variables,
    };

    LogupRoundPolynomial {
        layer: PolynomialLayer::CircuitLayer(output_layer),
        eq_row,
        eq_interaction: poly.eq_interaction,
        lambda: poly.lambda,
        point: poly.point,
        eq_adjustment: Ext::one(),
        padding_adjustment,
    }
}

async fn fix_last_variable_materialized_round(
    mut poly: LogupRoundPolynomial,
    alpha: Ext,
) -> LogupRoundPolynomial {
    // Remove the last coordinate from the point
    let last_coordinate = poly.point.remove_last_coordinate();
    let padding_adjustment = poly.padding_adjustment
        * (last_coordinate * alpha + (Ext::one() - last_coordinate) * (Ext::one() - alpha));

    match &poly.layer {
        PolynomialLayer::InteractionsLayer(guts) => {
            let height = guts.sizes()[1];
            let output_height = height.div_ceil(2);
            let backend = guts.backend();

            let mut output = Tensor::with_sizes_in([4, output_height], backend.clone());

            const BLOCK_SIZE: usize = 256;
            const STRIDE: usize = 32;
            let grid_size_x = height.div_ceil(BLOCK_SIZE * STRIDE);
            let grid_size = (grid_size_x, 1, 1);

            unsafe {
                let args = args!(guts.as_ptr(), output.as_mut_ptr(), alpha, height, output_height);
                output.assume_init();
                backend
                    .launch_kernel(
                        fix_last_variable_interactions_layer_kernel(),
                        grid_size,
                        BLOCK_SIZE,
                        &args,
                        0,
                    )
                    .unwrap();
            }

            let layer = PolynomialLayer::InteractionsLayer(output);

            let eq_interaction = poly.eq_interaction.fix_last_variable(alpha).await;

            LogupRoundPolynomial {
                layer,
                eq_row: poly.eq_row,
                eq_interaction,
                lambda: poly.lambda,
                point: poly.point,
                eq_adjustment: poly.eq_adjustment,
                padding_adjustment,
            }
        }
        PolynomialLayer::CircuitLayer(circuit) => {
            let backend = circuit.jagged_mle.backend();
            let height = circuit.jagged_mle.dense_data.height;
            // If this is the last layer, we need to fix the last variable and create an
            // interaction layer.
            if circuit.num_row_variables == 1 {
                let height = height >> 1;
                let mut output: Tensor<Ext, TaskScope> =
                    Tensor::with_sizes_in([4, height], backend.clone());

                const BLOCK_SIZE: usize = 256;
                const STRIDE: usize = 32;
                let stride = height.div_ceil(STRIDE);
                let grid_size_x = height.div_ceil(BLOCK_SIZE * stride);
                let grid_size = (grid_size_x, 1, 1);
                unsafe {
                    let args = args!(circuit.jagged_mle.dense_data.as_ptr(), output.as_mut_ptr());
                    output.assume_init();
                    backend
                        .launch_kernel(
                            fix_last_row_last_circuit_layer_kernel(),
                            grid_size,
                            BLOCK_SIZE,
                            &args,
                            0,
                        )
                        .unwrap();
                }
                let eq_row = poly.eq_row.fix_last_variable(alpha).await;

                return LogupRoundPolynomial {
                    layer: PolynomialLayer::InteractionsLayer(output),
                    eq_row,
                    eq_interaction: poly.eq_interaction,
                    lambda: poly.lambda,
                    point: poly.point,
                    eq_adjustment: padding_adjustment,
                    padding_adjustment: Ext::one(),
                };
            }
            unreachable!();
        }
    }
}

async fn finalize_univariate(
    poly: &LogupRoundPolynomial,
    univariate_evals: Tensor<Ext, TaskScope>,
    claim: Ext,
) -> UnivariatePolynomial<Ext> {
    let evals = univariate_evals.sum(1).await.into_buffer().to_host().await.unwrap();
    let mut eval_zero: Ext = *evals[0];
    let mut eval_half: Ext = *evals[1];
    let eq_sum = *evals[2];

    // Correct the evaluations by the sum of the eq polynomial, which accounts for the
    // contribution of padded row for the denominator expression
    // `\Sum_i eq * denominator_0 * denominator_1`.
    let eq_correction_term = poly.padding_adjustment - eq_sum;
    // The evaluation at zero just gets the eq correction term.
    eval_zero += eq_correction_term * (Ext::one() - *poly.point.last().unwrap());
    // The evaluation at 1/2 gets the eq correction term times 4, since the denominators
    // have a 1/2 in them for the rest of the evaluations (so we multiply by 2 twice).
    eval_half += eq_correction_term * Ext::from_canonical_u16(4);

    // Since the sumcheck polynomial is homogeneous of degree 3, we need to divide by
    // 8 = 2^3 to account for the evaluations at 1/2 to be double their true value.
    let eval_half = eval_half * Ext::from_canonical_u16(8).inverse();

    let eval_zero = eval_zero * poly.eq_adjustment;
    let eval_half = eval_half * poly.eq_adjustment;

    // Get the root of the eq polynomial which gives an evaluation of zero.
    let point_last = poly.point.last().unwrap();
    let b_const = (Ext::one() - *point_last) / (Ext::one() - point_last.double());

    let eval_one = claim - eval_zero;

    interpolate_univariate_polynomial(
        &[
            Ext::from_canonical_u16(0),
            Ext::from_canonical_u16(1),
            Ext::from_canonical_u16(2).inverse(),
            b_const,
        ],
        &[eval_zero, eval_one, eval_half, Ext::zero()],
    )
}

async fn sum_as_poly_materialized_round(
    poly: &LogupRoundPolynomial,
    claim: Ext,
) -> UnivariatePolynomial<Ext> {
    let univariate_evals = match &poly.layer {
        PolynomialLayer::CircuitLayer(circuit) => {
            let height = circuit.jagged_mle.dense_data.height;
            let scope = circuit.jagged_mle.backend();

            const BLOCK_SIZE: usize = 256;
            const STRIDE: usize = 32;
            let grid_dim = height.div_ceil(BLOCK_SIZE).div_ceil(STRIDE);
            let mut output = Tensor::<Ext, TaskScope>::with_sizes_in([3, grid_dim], scope.clone());
            let num_tiles = BLOCK_SIZE.checked_div(STRIDE).unwrap_or(1);
            let shared_mem = num_tiles * std::mem::size_of::<Ext>();
            unsafe {
                let kernel = if poly.eq_row.guts().total_len() == 2 {
                    first_sum_as_poly_layer_circuit_layer_kernel()
                } else {
                    sum_as_poly_circuit_layer_kernel()
                };
                output.assume_init();
                let args = args!(
                    output.as_mut_ptr(),
                    circuit.jagged_mle.as_raw(),
                    poly.eq_row.guts().as_ptr(),
                    poly.eq_interaction.guts().as_ptr(),
                    poly.lambda
                );
                scope.launch_kernel(kernel, grid_dim, BLOCK_SIZE, &args, shared_mem).unwrap();
            }
            output
        }
        PolynomialLayer::InteractionsLayer(_guts) => {
            unreachable!("first sum_as_poly should always be circuit layer")
        }
    };

    finalize_univariate(poly, univariate_evals, claim).await
}

// returns (next univariate, next round polynomial)
async fn fix_and_sum_materialized_round(
    mut poly: LogupRoundPolynomial,
    alpha: Ext,
    claim: Ext,
) -> (UnivariatePolynomial<Ext>, LogupRoundPolynomial) {
    // Remove the last coordinate from the point
    let last_coordinate = poly.point.remove_last_coordinate();
    let padding_adjustment = poly.padding_adjustment
        * (last_coordinate * alpha + (Ext::one() - last_coordinate) * (Ext::one() - alpha));

    match &poly.layer {
        PolynomialLayer::InteractionsLayer(guts) => {
            // First, fix_last_variable on the eq_interaction
            let eq_interaction = poly.eq_interaction.fix_last_variable(alpha).await;
            let height = guts.sizes()[1];
            let output_height = height.div_ceil(2);
            let backend = guts.backend();

            let mut output = Tensor::with_sizes_in([4, output_height], backend.clone());

            const BLOCK_SIZE: usize = 256;
            const STRIDE: usize = 32;
            let grid_size_x = height.div_ceil(BLOCK_SIZE).div_ceil(STRIDE);
            let grid_size = (grid_size_x, 1, 1);
            let mut univariate_evals =
                Tensor::<Ext, TaskScope>::with_sizes_in([3, grid_size_x], backend.clone());
            let num_tiles = BLOCK_SIZE.checked_div(32).unwrap_or(1);
            let shared_mem = num_tiles * std::mem::size_of::<Ext>();

            unsafe {
                univariate_evals.assume_init();
                output.assume_init();
                let args = args!(
                    univariate_evals.as_mut_ptr(),
                    guts.as_ptr(),
                    output.as_mut_ptr(),
                    alpha,
                    height,
                    output_height,
                    eq_interaction.guts().as_ptr(),
                    poly.lambda
                );
                backend
                    .launch_kernel(
                        fix_and_sum_interactions_layer_kernel(),
                        grid_size,
                        BLOCK_SIZE,
                        &args,
                        shared_mem,
                    )
                    .unwrap();
            }

            let layer = PolynomialLayer::InteractionsLayer(output);

            let poly = LogupRoundPolynomial {
                layer,
                eq_row: poly.eq_row,
                eq_interaction,
                lambda: poly.lambda,
                point: poly.point,
                eq_adjustment: poly.eq_adjustment,
                padding_adjustment,
            };
            let univariate_evals = finalize_univariate(&poly, univariate_evals, claim).await;
            (univariate_evals, poly)
        }
        PolynomialLayer::CircuitLayer(circuit) => {
            let backend = circuit.jagged_mle.backend();
            let height = circuit.jagged_mle.dense_data.height;
            // If this is the last layer, we need to fix the last variable and create an
            // interaction layer.
            if circuit.num_row_variables == 1 {
                let height = height >> 1;
                let mut output: Tensor<Ext, TaskScope> =
                    Tensor::with_sizes_in([4, height], backend.clone());

                let eq_row = poly.eq_row.fix_last_variable(alpha).await;

                const BLOCK_SIZE: usize = 256;
                const STRIDE: usize = 32;
                let grid_size_x = height.div_ceil(BLOCK_SIZE).div_ceil(STRIDE);
                let grid_size = (grid_size_x, 1, 1);
                let mut univariate_evals =
                    Tensor::<Ext, TaskScope>::with_sizes_in([3, grid_size_x], backend.clone());
                let num_tiles = BLOCK_SIZE.checked_div(32).unwrap_or(1);
                let shared_mem = num_tiles * std::mem::size_of::<Ext>();

                unsafe {
                    univariate_evals.assume_init();
                    output.assume_init();
                    let args = args!(
                        univariate_evals.as_mut_ptr(),
                        circuit.jagged_mle.dense_data.as_ptr(),
                        alpha,
                        output.as_mut_ptr(),
                        poly.eq_interaction.guts().as_ptr(),
                        poly.lambda
                    );
                    backend
                        .launch_kernel(
                            fix_and_sum_last_circuit_layer_kernel(),
                            grid_size,
                            BLOCK_SIZE,
                            &args,
                            shared_mem,
                        )
                        .unwrap();
                }
                let poly = LogupRoundPolynomial {
                    layer: PolynomialLayer::InteractionsLayer(output),
                    eq_row,
                    eq_interaction: poly.eq_interaction,
                    lambda: poly.lambda,
                    point: poly.point,
                    eq_adjustment: padding_adjustment,
                    padding_adjustment: Ext::one(),
                };
                let univariate_evals = finalize_univariate(&poly, univariate_evals, claim).await;
                (univariate_evals, poly)
            } else {
                let eq_row_handle = tokio::spawn(async move {
                    let eq_row = poly.eq_row.fix_last_variable(alpha).await;
                    eq_row
                });
                let output_interaction_col_sizes = circuit
                    .interaction_col_sizes
                    .iter()
                    .map(|count| count.div_ceil(4) * 2) // Make sure that the row counts are always even.
                    .collect::<Vec<_>>();
                // The output indices is just the prefix sum of the interaction row counts.
                let output_interaction_start_indices = once(0)
                    .chain(output_interaction_col_sizes.iter().scan(0u32, |acc, x| {
                        *acc += x;
                        Some(*acc)
                    }))
                    .collect::<Buffer<_>>();
                let output_height =
                    output_interaction_start_indices.last().copied().unwrap() as usize;
                let output_interaction_start_indices =
                    output_interaction_start_indices.to_device_in(backend).await.unwrap();

                // Create a new layer
                let output_layer: Tensor<Ext, TaskScope> =
                    Tensor::with_sizes_in([4, 1, output_height * 2], backend.clone());
                let output_col_index: Buffer<u32, TaskScope> =
                    Buffer::with_capacity_in(output_height, backend.clone());

                let output_jagged_layer = JaggedGkrLayer::new(output_layer, output_height);
                let mut output_jagged_mle = JaggedMle::new(
                    output_jagged_layer,
                    output_col_index,
                    output_interaction_start_indices,
                );

                // populate the new layer
                const BLOCK_SIZE: usize = 256;
                const STRIDE: usize = 32;
                let grid_size_x = height.div_ceil(BLOCK_SIZE).div_ceil(STRIDE);
                let grid_size = (grid_size_x, 1, 1);
                let block_dim = BLOCK_SIZE;

                let mut univariate_evals =
                    Tensor::<Ext, TaskScope>::with_sizes_in([3, grid_size_x], backend.clone());
                let num_tiles = BLOCK_SIZE.checked_div(32).unwrap_or(1);
                let shared_mem = num_tiles * std::mem::size_of::<Ext>();
                let eq_row = eq_row_handle.await.unwrap();

                unsafe {
                    univariate_evals.assume_init();
                    output_jagged_mle.dense_data.assume_init();
                    output_jagged_mle.col_index.assume_init();
                    let args = args!(
                        univariate_evals.as_mut_ptr(),
                        circuit.jagged_mle.as_raw(),
                        output_jagged_mle.as_mut_raw(),
                        alpha,
                        eq_row.guts().as_ptr(),
                        poly.eq_interaction.guts().as_ptr(),
                        poly.lambda
                    );
                    backend
                        .launch_kernel(
                            fix_and_sum_circuit_layer_kernel(),
                            grid_size,
                            block_dim,
                            &args,
                            shared_mem,
                        )
                        .unwrap();
                }

                let output_layer = GkrLayer {
                    jagged_mle: output_jagged_mle,
                    interaction_col_sizes: output_interaction_col_sizes,
                    num_row_variables: circuit.num_row_variables - 1,
                    num_interaction_variables: circuit.num_interaction_variables,
                };

                let poly = LogupRoundPolynomial {
                    layer: PolynomialLayer::CircuitLayer(output_layer),
                    eq_row,
                    eq_interaction: poly.eq_interaction,
                    lambda: poly.lambda,
                    point: poly.point,
                    eq_adjustment: poly.eq_adjustment,
                    padding_adjustment,
                };

                let univariate = finalize_univariate(&poly, univariate_evals, claim).await;
                (univariate, poly)
            }
        }
    }
}

/// Process a univariate polynomial by observing it with the challenger and sampling the next evaluation point
#[inline]
fn process_univariate_polynomial<C>(
    uni_poly: UnivariatePolynomial<Ext>,
    challenger: &mut C,
    univariate_poly_msgs: &mut Vec<UnivariatePolynomial<Ext>>,
    point: &mut Vec<Ext>,
) -> Ext
where
    C: FieldChallenger<Felt>,
{
    let coefficients =
        uni_poly.coefficients.iter().flat_map(|x| x.as_base_slice()).copied().collect_vec();
    challenger.observe_slice(&coefficients);
    univariate_poly_msgs.push(uni_poly);
    let alpha: Ext = challenger.sample_ext_element();
    point.insert(0, alpha);
    alpha
}

pub async fn first_round_sumcheck<C>(
    poly: FirstLayerPolynomial,
    challenger: &mut C,
    claim: Ext,
) -> (PartialSumcheckProof<Ext>, Vec<Ext>)
where
    C: FieldChallenger<Felt>,
{
    // Check that all the polynomials have the same number of variables.
    let num_variables = poly.num_variables();

    // The first round will process the first t variables, so we need to ensure that there are at least t variables.
    assert!(num_variables >= 1_u32);

    // The point at which the reduced sumcheck proof should be evaluated.
    let mut point = vec![];

    // The univariate poly messages.  This will be a rlc of the polys' univariate polys.
    let mut univariate_poly_msgs: Vec<UnivariatePolynomial<Ext>> = vec![];

    let mut uni_poly = sum_as_poly_first_layer(&poly, claim).await;

    let mut alpha = process_univariate_polynomial(
        uni_poly.clone(),
        challenger,
        &mut univariate_poly_msgs,
        &mut point,
    );

    let mut poly = fix_last_variable_first_layer(poly, alpha).await;

    let round_claim = uni_poly.eval_at_point(*point.first().unwrap());

    uni_poly = sum_as_poly_materialized_round(&poly, round_claim).await;

    alpha =
        process_univariate_polynomial(uni_poly, challenger, &mut univariate_poly_msgs, &mut point);

    for _ in 2..num_variables as usize {
        // Get the round claims from the last round's univariate poly messages.
        let round_claim = univariate_poly_msgs.last().unwrap().eval_at_point(alpha);

        (uni_poly, poly) = fix_and_sum_materialized_round(poly, alpha, round_claim).await;

        alpha = process_univariate_polynomial(
            uni_poly,
            challenger,
            &mut univariate_poly_msgs,
            &mut point,
        );
    }

    poly = fix_last_variable_materialized_round(poly, *point.first().unwrap()).await;

    let evals = univariate_poly_msgs.last().unwrap().eval_at_point(*point.first().unwrap());

    let component_poly_evals = get_component_poly_evals(&poly).await;

    (
        PartialSumcheckProof {
            univariate_polys: univariate_poly_msgs,
            claimed_sum: claim,
            point_and_eval: (point.into(), evals),
        },
        component_poly_evals,
    )
}

pub async fn materialized_round_sumcheck<C>(
    mut poly: LogupRoundPolynomial,
    challenger: &mut C,
    claim: Ext,
) -> (PartialSumcheckProof<Ext>, Vec<Ext>)
where
    C: FieldChallenger<Felt>,
{
    let num_variables = poly.num_variables();
    assert!(num_variables >= 1_u32);

    let mut point = Vec::with_capacity(num_variables as usize);
    let mut univariate_poly_msgs = Vec::with_capacity(num_variables as usize);

    // First round: compute initial univariate polynomial
    let uni_poly = sum_as_poly_materialized_round(&poly, claim).await;
    let alpha =
        process_univariate_polynomial(uni_poly, challenger, &mut univariate_poly_msgs, &mut point);

    // Early return for single variable case
    if num_variables == 1 {
        poly = fix_last_variable_materialized_round(poly, alpha).await;
        let eval = univariate_poly_msgs[0].eval_at_point(alpha);
        let component_poly_evals = get_component_poly_evals(&poly).await;

        return (
            PartialSumcheckProof {
                univariate_polys: univariate_poly_msgs,
                claimed_sum: claim,
                point_and_eval: (point.into(), eval),
            },
            component_poly_evals,
        );
    }

    // Process remaining rounds
    let mut round_claim = univariate_poly_msgs[0].eval_at_point(alpha);

    for _round in 1..num_variables as usize {
        let (uni_poly, next_poly) =
            fix_and_sum_materialized_round(poly, point[0], round_claim).await;
        poly = next_poly;

        let alpha = process_univariate_polynomial(
            uni_poly,
            challenger,
            &mut univariate_poly_msgs,
            &mut point,
        );
        round_claim = univariate_poly_msgs.last().unwrap().eval_at_point(alpha);
    }

    // Final fix_last_variable
    poly = fix_last_variable_materialized_round(poly, point[0]).await;

    // Compute final evaluation
    let eval = univariate_poly_msgs.last().unwrap().eval_at_point(point[0]);
    let component_poly_evals = get_component_poly_evals(&poly).await;

    (
        PartialSumcheckProof {
            univariate_polys: univariate_poly_msgs,
            claimed_sum: claim,
            point_and_eval: (point.into(), eval),
        },
        component_poly_evals,
    )
}

pub async fn bench_materialized_sumcheck<R: rand::Rng>(
    interaction_col_sizes: Vec<u32>,
    rng: &mut R,
    num_row_variables: Option<u32>,
) {
    type Config = Poseidon2KoalaBear16BasefoldConfig;
    let verifier = BasefoldVerifier::<_, Config>::new(1);
    let get_challenger = move || verifier.clone().challenger();
    let now = std::time::Instant::now();

    let (layer, test_data) =
        generate_test_data(rng, interaction_col_sizes, num_row_variables).await;

    println!("generate test data took {}s", now.elapsed().as_secs_f64());

    let GkrTestData { numerator_0, numerator_1, denominator_0, denominator_1 } = test_data;

    let GkrLayer {
        jagged_mle,
        interaction_col_sizes,
        num_interaction_variables,
        num_row_variables,
    } = layer;

    println!("num_row_variables: {num_row_variables}");
    println!("num_interaction_variables: {num_interaction_variables}");
    let poly_point = Point::<Ext>::rand(rng, num_row_variables + num_interaction_variables);
    let (interaction_point, row_point) = poly_point.split_at(num_interaction_variables as usize);

    let lambda = rng.gen::<Ext>();

    csl_cuda::spawn(move |t| async move {
        let now = std::time::Instant::now();
        let jagged_mle = jagged_mle.into_device(&t).await.unwrap();

        let row_point = row_point.to_device_in(&t).await.unwrap();
        let interaction_point = interaction_point.to_device_in(&t).await.unwrap();

        let eq_row = Mle::partial_lagrange(&row_point).await;
        let eq_interaction = Mle::partial_lagrange(&interaction_point).await;

        println!("moving to device took {}s", now.elapsed().as_secs_f64());

        let layer = GkrLayer {
            jagged_mle,
            interaction_col_sizes,
            num_interaction_variables,
            num_row_variables,
        };

        let polynomial = LogupRoundPolynomial {
            layer: PolynomialLayer::CircuitLayer(layer),
            eq_row,
            eq_interaction,
            lambda,
            eq_adjustment: Ext::one(),
            padding_adjustment: Ext::one(),
            point: poly_point.clone(),
        };

        let host_eq = Mle::partial_lagrange(&poly_point).await;
        let now = std::time::Instant::now();
        let claim = slop_futures::rayon::spawn(move || {
            numerator_0
                .guts()
                .as_slice()
                .par_iter()
                .zip_eq(numerator_1.guts().as_slice().par_iter())
                .zip_eq(denominator_0.guts().as_slice().par_iter())
                .zip_eq(denominator_1.guts().as_slice().par_iter())
                .zip_eq(host_eq.guts().as_slice().par_iter())
                .map(|((((n_0, n_1), d_0), d_1), eq)| {
                    let numerator_eval = *n_0 * *d_1 + *n_1 * *d_0;
                    let denominator_eval = *d_0 * *d_1;
                    *eq * (numerator_eval * lambda + denominator_eval)
                })
                .sum::<Ext>()
        })
        .await
        .unwrap();

        let mut challenger = get_challenger();
        t.synchronize().await.unwrap();
        println!(
            "time for claim on host is {}, now starting sumcheck",
            now.elapsed().as_secs_f64()
        );

        let now = std::time::Instant::now();
        let (mut proof, mut evals) =
            materialized_round_sumcheck(polynomial.clone(), &mut challenger, claim).await;
        println!("time for sumcheck: {}", now.elapsed().as_secs_f64());

        for _ in 0..2 {
            let now = std::time::Instant::now();
            t.synchronize().await.unwrap();
            let mut challenger = get_challenger();
            (proof, evals) =
                materialized_round_sumcheck(polynomial.clone(), &mut challenger, claim).await;
            println!("time for sumcheck: {}", now.elapsed().as_secs_f64());
        }

        let mut challenger = get_challenger();
        partially_verify_sumcheck_proof(
            &proof,
            &mut challenger,
            (num_row_variables + num_interaction_variables) as usize,
            3,
        )
        .unwrap();

        let (point, expected_final_eval) = proof.point_and_eval;

        // Assert that the point has the expected dimension.
        assert_eq!(point.dimension() as u32, num_row_variables + num_interaction_variables);

        // Calculate the expected evaluations at the point.
        let [n_0, n_1, d_0, d_1] = evals.try_into().unwrap();
        let eq_eval = Mle::full_lagrange_eval(&poly_point, &point);

        let expected_numerator_eval = n_0 * d_1 + n_1 * d_0;
        let expected_denominator_eval = d_0 * d_1;
        let eval = expected_numerator_eval * lambda + expected_denominator_eval;
        let final_eval = eq_eval * eval;

        // Assert that the final eval is correct.
        assert_eq!(final_eval, expected_final_eval);
    })
    .await
    .unwrap();
}

#[cfg(test)]
mod tests {
    use crate::logup_gkr::utils::{generate_test_data, GkrTestData};
    use slop_multilinear::Mle;
    use slop_multilinear::Point;

    use super::*;

    use rand::{rngs::StdRng, Rng, SeedableRng as _};

    /// Since we don't ever *only* fix last variable on a normal circuit layer, this unit test does fix_and_sum with a dummy claim.q
    #[tokio::test]
    async fn test_logup_round_polynomial_fix_last_variable() {
        let mut rng = StdRng::seed_from_u64(0);

        let interaction_col_sizes: Vec<u32> =
            vec![(1 << 8) + 2, (1 << 10) + 2, 1 << 8, 1 << 6, 1 << 10, 1 << 8, (1 << 6) + 2];
        let (layer, test_data) = generate_test_data(&mut rng, interaction_col_sizes, None).await;
        let GkrTestData { numerator_0, numerator_1, denominator_0, denominator_1 } = test_data;

        let GkrLayer {
            jagged_mle,
            interaction_col_sizes,
            num_interaction_variables,
            num_row_variables,
        } = layer;

        let poly_point =
            Point::<Ext>::rand(&mut rng, num_row_variables + num_interaction_variables + 1);
        let (interaction_point, row_point) =
            poly_point.split_at(num_interaction_variables as usize);

        let random_point =
            Point::<Ext>::rand(&mut rng, num_row_variables + num_interaction_variables);

        let lambda = rng.gen::<Ext>();

        csl_cuda::spawn(move |t| async move {
            let jagged_mle = jagged_mle.into_device(&t).await.unwrap();

            let row_point = row_point.to_device_in(&t).await.unwrap();
            let interaction_point = interaction_point.to_device_in(&t).await.unwrap();

            let eq_row = Mle::partial_lagrange(&row_point).await;
            let eq_interaction = Mle::partial_lagrange(&interaction_point).await;

            let layer = GkrLayer {
                jagged_mle,
                interaction_col_sizes,
                num_interaction_variables,
                num_row_variables,
            };

            let mut polynomial = LogupRoundPolynomial {
                layer: PolynomialLayer::CircuitLayer(layer),
                eq_row,
                eq_interaction,
                lambda,
                eq_adjustment: Ext::one(),
                padding_adjustment: Ext::one(),
                point: poly_point,
            };

            // Get the exepcted evaluations
            let numerator_0_eval = numerator_0.eval_at(&random_point).await[0];
            let numerator_1_eval = numerator_1.eval_at(&random_point).await[0];
            let denominator_0_eval = denominator_0.eval_at(&random_point).await[0];
            let denominator_1_eval = denominator_1.eval_at(&random_point).await[0];

            for alpha in random_point.iter().rev() {
                let _uni_poly;
                (_uni_poly, polynomial) =
                    fix_and_sum_materialized_round(polynomial, *alpha, Ext::zero()).await;
            }
            let component_poly_evals = get_component_poly_evals(&polynomial).await;

            // Get the values from the sumcheck polynomial
            let [n_0, n_1, d_0, d_1] = component_poly_evals.try_into().unwrap();
            assert_eq!(numerator_0_eval, n_0);
            assert_eq!(numerator_1_eval, n_1);
            assert_eq!(denominator_0_eval, d_0);
            assert_eq!(denominator_1_eval, d_1);
        })
        .await
        .unwrap();
    }

    #[tokio::test]
    async fn test_logup_round_sumcheck_polynomial() {
        let mut rng = StdRng::seed_from_u64(0);
        // let interaction_col_sizes: Vec<u32> = vec![
        //     14216, 14216, 14216, 14216, 14216, 14216, 14216, 14216, 14216, 14216, 14216, 14216,
        //     14216, 14216, 14216, 14216, 14216, 14216, 14216, 14216, 14216, 362856, 362856, 362856,
        //     362856, 362856, 362856, 362856, 362856, 362856, 362856, 362856, 362856, 362856, 362856,
        //     362856, 362856, 362856, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312,
        //     312, 312, 312, 312, 312, 312, 312, 312, 129480, 129480, 129480, 129480, 129480, 129480,
        //     129480, 129480, 129480, 129480, 129480, 129480, 129480, 129480, 129480, 129480, 129480,
        //     129480, 129480, 129480, 129480, 129480, 129480, 129480, 129480, 185848, 185848, 185848,
        //     185848, 185848, 185848, 185848, 185848, 185848, 185848, 185848, 185848, 185848, 185848,
        //     185848, 185848, 185848, 185848, 185848, 16384, 16384, 16384, 16384, 16384, 16384, 4, 4,
        //     4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        //     4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        //     4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        //     4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
        //     4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 123640, 123640, 123640, 123640,
        //     123640, 123640, 123640, 3360, 3360, 3360, 3360, 3360, 3360, 3360, 3360, 3360, 3360,
        //     3360, 3360, 3360, 3360, 3360, 3360, 3360, 35400, 35400, 35400, 35400, 35400, 35400,
        //     35400, 35400, 35400, 35400, 35400, 35400, 35400, 35400, 35400, 35400, 35400, 35400,
        //     35400, 35400, 35400, 162272, 162272, 162272, 162272, 162272, 162272, 162272, 162272,
        //     162272, 162272, 162272, 162272, 162272, 162272, 162272, 162272, 162272, 162272, 162272,
        //     162272, 162272, 162272, 162272, 172136, 172136, 172136, 172136, 172136, 172136, 172136,
        //     172136, 172136, 172136, 172136, 172136, 172136, 172136, 172136, 172136, 172136, 172136,
        //     172136, 172136, 172136, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
        //     200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 2472, 2472, 2472, 2472, 2472, 2472,
        //     2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472, 2472,
        //     2472, 2472, 2384, 2384, 2384, 2384, 2384, 2384, 2384, 2384, 2384, 2384, 2384, 2384,
        //     2384, 2384, 2384, 2384, 2384, 2384, 2384, 2384, 2384, 4568, 4568, 4568, 4568, 4568,
        //     4568, 4568, 4568, 4568, 4568, 4568, 4568, 4568, 4568, 4568, 4568, 4568, 4568, 4568,
        //     4568, 16, 16, 16, 16, 16, 16, 16, 61824, 61824, 61824, 61824, 61824, 61824, 61824,
        //     61824, 61824, 61824, 61824, 61824, 61824, 61824, 32, 32, 32, 32, 32, 32, 32, 32, 32,
        //     32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,
        //     32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,
        //     118008, 32768, 44304, 44304, 44304, 44304, 44304, 44304, 44304, 44304, 44304, 44304,
        //     44304, 44304, 44304, 44304, 44304, 44304, 44304, 44304, 44304, 44304, 44304, 44304,
        //     44304, 44304, 44304, 44304, 44304, 6928, 6928, 6928, 6928, 6928, 6928, 6928, 6928,
        //     6928, 6928, 6928, 6928, 6928, 6928, 6928, 6928, 6928, 6928, 6928, 6928, 6928, 6928,
        //     6928, 6928, 6928, 6928, 6928, 6928, 6928, 8, 8, 8, 8, 8, 8, 8, 8, 117224, 117224,
        //     117224, 117224, 117224, 117224, 117224, 117224, 117224, 117224, 117224, 117224, 117224,
        //     117224, 117224, 117224, 117224, 117224, 117224, 117224, 117224, 117224, 117224, 181136,
        //     181136, 181136, 181136, 181136, 181136, 181136, 181136, 181136, 181136, 181136, 181136,
        //     181136, 181136, 181136, 181136, 181136, 181136, 181136, 181136, 181136, 3032, 3032,
        //     3032, 3032, 3032, 3032, 3032, 3032, 3032, 3032, 3032, 3032, 3032, 3032, 3032, 3032,
        //     3032, 3032, 3032, 3032, 3032, 2640, 2640, 2640, 2640, 2640, 2640, 2640, 2640, 2640,
        //     2640, 2640, 2640, 2640, 2640, 2640, 2640, 2640, 2640, 2640, 2640, 2640, 4648, 4648,
        //     4648, 4648, 4648, 4648, 4648, 4648, 4648, 4648, 4648, 4648, 4648, 4648, 4648, 4648,
        //     4648, 4648, 4648, 4648, 4648, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        //     8, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
        //     8, 8, 8, 8, 8, 8, 29160, 29160, 29160, 29160, 29160, 29160, 29160, 29160, 29160, 29160,
        //     29160, 29160, 29160,
        // ];
        let interaction_col_sizes: Vec<u32> = vec![92, 100, 278, 220, 82, 82];

        bench_materialized_sumcheck(interaction_col_sizes, &mut rng, None).await;
    }
}
